<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tongxuan Tian</title>

    <meta name="author" content="Tongxuan Tian">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/android-chrome-512x512_02.png"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  </head>

  <body>
    <!-- <div style="height: 10px;"></div> -->
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:65%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Tongxuan Tian
                </p>
                <p>
                  Hi, My name is Tongxuan Tian. I am now a research intern at Stanford <a href="https://tml.stanford.edu/"></a> <a href="https://tml.stanford.edu/">The Movement Lab</a> advised by Prof. <a href="https://tml.stanford.edu/people/karen-liu" >Karen Liu</a>.
                  I hold my Master's degree in Computer Science from the <a href="https://www.virginia.edu/">University of Virginia</a> and Bachelor's degree from <a href="https://www.tongji.edu.cn/eng/">Tongji University</a>.
                </p>
                <p>
                  Previously, I was a visiting student at <a href="https://ucsd.edu/">UC San Digo</a> SuLab, where I was advised by Prof. <a href="https://cseweb.ucsd.edu/~haosu/index.html">Hao Su</a>.
                  During my Master's studies at UVa, I started my robotics research in the <a href="https://github.com/live-robotics-uva">LIVE Robotics Lab</a> under the supervision of Prof. <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a>.
                  I was fortunate to collaborate with Prof. <a href="https://jzengust.github.io/">Jin Zeng</a> at Tongji University and Dr. <a href="http://wenxiusun.com/">Wenxiu Sun</a> at Sensetime Research.
                </p>
                <p>
                  <strong>Contact:</strong> tongxuan259 [at] gmail (dot) com
                </p>
                <p style="text-align:center" class="social-icons">
                  <!-- <a href="data/Tongxuan_Tian_UVA .pdf" style="font-size: 2.5em; font-weight: bold;">CV</a> -->
                  <a href="https://scholar.google.com/citations?user=-X-FdfgAAAAJ&hl=en" title="Google Scholar"><i class="ai ai-google-scholar" style="font-size: 2.5em;"></i></a>
                  <a href="https://github.com/Tongxuan259" title="GitHub"><i class="fa-brands fa-github" style="font-size: 2.5em;"></i></a>
                  <a href="https://x.com/txtian_259" title="Twitter"><i class="fa-brands fa-twitter" style="font-size: 2.5em;"></i></a>
                  <a href="https://www.linkedin.com/in/tongxuan-tian/" title="LinkedIn"><i class="fa-brands fa-linkedin" style="font-size: 2.5em;"></i></a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:35%">
                <a href="images/potrait/photo_02.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/potrait/photo_02.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <!-- <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 0 0 20px 0;"> -->
                <!-- <h2 style="font-size: 28px;">Research</h2>
                <p>
                  I work on building general purpose robotic system.s


                </p> -->
                <p class="research-interest-title"><strong>Research Interests:</strong></p>
                <p class="research-interest">1. Dexterous hand manipulation and loco-manipulation.</p>
                <p class="research-interest">2. Multimodal learning including language, vision and touch.</p>
                <p class="research-interest">3. Cognitive architecture for embodied intelligence.</p>
                <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 20px 0 20px 0;">
                <h2 style="font-size: 28px;">Selected Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <!-- 
            1.research interest
            2. seletec publication
            3. experience
            5. honors
            6. service
            7. teaching
          -->

          


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            
            <!-- Force-Controlled Gripper Paper -->
            <tr style="padding:0px">
              <td style="padding:20px;width:35%;vertical-align:middle">
                <div style="text-align: center;">
                  <img src='images/publication/force-gripper.png' style="width:120%;max-width:110%;height:100%;border-radius:8px;box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                </div>
              </td>
              <td style="padding:20px;width:65%;vertical-align:middle">
                <a href="https://force-gripper.github.io/">
                  <span class="papertitle">Universal Low-Cost Force-Controlled Gripper for Learning Delicate Object Grasping</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=sIY8FtgAAAAJ&hl=en">Xuhui Kang</a><sup>*</sup>,
                <strong>Tongxuan Tian</strong><sup>*</sup>,
                <a href="https://seansungwooklee.github.io/">Sung-Wook Lee</a><sup></sup>,
                <a href="https://binghao-huang.github.io/#intro">Binghao Huang</a><sup></sup>,
                <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a><sup></sup>
                <br>
                <em>In Submission</em>, 2025
      
                <br>
                <a href="https://force-gripper.github.io/">project page</a>
                /
                <a href="https://force-gripper.github.io/">arXiv</a>
                <br>
                <p style="margin-top:5px;">
                  <strong>TL;DR:</strong> A versatile and affordable force-controlled gripper with tactile sensors for delicate object manipulation tasks.
                </p>
              </td>
            </tr>
            
            
            <!-- O3Afford Paper -->
            <tr style="padding:0px">
              <td style="padding:20px;width:35%;vertical-align:middle">
                <div style="text-align: center;">
                  <img src='images/publication/o3afford.gif' style="width:100%;max-width:100%;height:auto;border-radius:8px;box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                </div>
              </td>
              <td style="padding:20px;width:65%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">O³Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</span>
                </a>
                <br>
                <strong>Tongxuan Tian</strong>,
                <a href="https://scholar.google.com/citations?user=sIY8FtgAAAAJ&hl=en">Xuhui Kang</a>,
                <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a>
                <br>
                <em>Conference on Robot Learning (CoRL)</em>, 2025
                <br>
                <a href="https://o3afford.github.io">project page</a>
                /
                <a href="https://arxiv.org/abs/2509.06233">arXiv</a>
                <br>
                <p style="margin-top:5px;">
                  <strong>TL;DR:</strong> A novel framework using vision foundation models for one-shot 3D affordance learning for robotic manipulation.
                </p>
              </td>
            </tr>

            <!-- UniClothDiff Paper -->
            <tr style="padding:0px">
              <td style="padding:20px;width:35%;vertical-align:middle">
                <div style="text-align: center;">
                  <img src='images/publication/diff_dynamics.gif' style="width:100%;max-width:100%;height:auto;border-radius:8px;box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                </div>
              </td>
              <td style="padding:20px;width:65%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">UniClothDiff: Diffusion Dynamics Model with Generative State Estimation for Cloth Manipulation</span>
                </a>
                <br>
                <strong>Tongxuan Tian*</strong>,
                <a href="https://haoyangli16.github.io/">Haoyang Li*</a>,
                <a href="https://albertboai.com/">Bo Ai</a>,
                <a href="https://rabbit-hu.github.io/">Xiaodi Yuan</a>,
                <a href="https://sites.google.com/view/zhiao-huang">Zhiao Huang</a>,
                <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>
                <br>
                <em>Conference on Robot Learning (CoRL)</em>, 2025
                <br>
                <a href="https://uniclothdiff.github.io">project page</a>
                /
                <a href="https://arxiv.org/abs/2503.11999">arXiv</a>
                <br>
                <p style="margin-top:5px;">
                  <strong>TL;DR:</strong> A novel approach for estimating and learning cloth state and dynamics with diffusion models for cloth manipulation.
                </p>
              </td>
            </tr>



            <!-- IJCV Paper -->
            <tr style="padding:0px">
              <td style="padding:20px;width:35%;vertical-align:middle">
                <div style="text-align: center;">
                  <img src='images/publication/IJCV.png' style="width:100%;max-width:100%;height:auto;border-radius:8px;box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                </div>
              </td>
              <td style="padding:20px;width:65%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">Deep Unrolled Weighted Graph Laplacian Regularization for Depth Completion</span>
                </a>
                <br>
                <a href="https://scholar.google.com.hk/citations?user=jBxf3FYAAAAJ&hl=zh-CN">Jin Zeng*</a>,
                <a href="https://scholar.google.com/citations?user=B7roEzMAAAAJ&hl=en">Qingpeng Zhu*<sup>†</sup></a>,
                <strong>Tongxuan Tian*</strong>,
                <a href="http://wenxiusun.com/">Wenxiu Sun</a>,
                <a href="https://cslinzhang.gitee.io/home/">Lin Zhang</a>,
                <a href="https://orcid.org/0000-0002-4301-394X">Shengjie Zhao</a>
                <br>
                <em>International Journal of Computer Vision (IJCV)</em>, 2024
                <br>
                <a href="https://link.springer.com/article/10.1007/s11263-024-02188-3">project page</a>
                /
                <a href="https://link.springer.com/article/10.1007/s11263-024-02188-3">arXiv</a>
                <br>
                <p style="margin-top:5px;">
                  <strong>TL;DR:</strong> We propose the deep unrolled Weighted Graph Laplacian Regularization (WGLR) for depth completion task which enforces input constraints in the network design.
                </p>
              </td>
            </tr>
          </tbody></table>

          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 0 0 20px 0;">
                <h2 style="font-size: 28px;">Experience</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id=''><video  width=100% muted autoplay loop>
                  <source src="images/experience/stanford.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/experience/stanford.png' style="width:80%;max-width:160px;display:block;margin:10px 20px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://tml.stanford.edu/">
                  <span class="papertitle">Stanford University - The Movement Lab</span>
                </a>
                <br>
                Research Intern, Supervisor: Prof. <a href="https://tml.stanford.edu/people/karen-liu">Karen Liu</a>
                <br>
                <em>Jun. 2025 - Present</em>
                <br>
                  • Led a project on dexterous hand manipulation with tactile sensing (Onging project).
                <br>
                  • Developed a general real2sim pipeline for tactile-based policy sim2real.
                <br>
                  • Developed a receipe for tasks that require active sensing with tactile.
              </td>
            </tr>

            <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id=''><video  width=100% muted autoplay loop>
                  <source src="images/experience/ucsd.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/experience/ucsd.png' style="width:80%;max-width:160px;display:block;margin:20px 20px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://cseweb.ucsd.edu/~haosu/">
                  <span class="papertitle">UC San Diego - SuLab</span>
                </a>
                <br>
                Visiting Student, Supervisor: Prof. <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>
                <br>
                <em>Jun. 2024 - Jun. 2025</em>
                <br>
                  • Led a project on cloth manipulation using generative world model (Accepted to CoRL 2025).
                <br>
                  • Developed a unifed pipeline for state estimation and dynamics modeling using diffusion models.
                <br>
                  • Led a project investigating the mechanisms of vision-language-action models to enhance steerability and enable customized action generation (Onging project).
              </td>
            </tr>

            <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id=''><video  width=100% muted autoplay loop>
                  <source src="images/experience/UVa.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/experience/UVa.png' style="width:70%;max-width:160px;display:block;margin:20px 30px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://github.com/live-robotics-uva">
                  <span class="papertitle">University of Virginia - LIVE Robotics Lab</span>
                </a>
                <br>
                Research Assistant, Supervisor: Prof. <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a>
                <br>
                <em>Aug. 2023 - May 2025</em>
                <br>
                  • Led research on object-to-object affordance grounding for robotic manipulation (Accepted to CoRL 2025).
                <br>
                  • Led a project developing a low-cost force–tactile gripper (~$200) that achieves performance comparable to commercial grippers.
              </td>
            </tr>

            <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id=''><video  width=100% muted autoplay loop>
                  <source src="images/experience/sensetime.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/experience/sensetime.png' style="width:80%;max-width:160px;display:block;margin:40px 20px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sensetime.com/">
                  <span class="papertitle">SenseTime Research</span>
                </a>
                <br>
                Research Intern, Supervisor: Dr. <a href="https://scholar.google.com/citations?user=B7roEzMAAAAJ&hl=en">Qingpeng Zhu</a>
                <br>
                <em>Oct. 2022 - Aug. 2023</em>
                <br>
                  • Developed deep unrolled weighted graph laplacian regularization methods for real-time and noise-robust depth completion.
                <br>
                  • Deployed the model on real-world sony sensor.
              </td>
            </tr>

          </tbody></table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 0 0 20px 0;">
                <h2 style="font-size: 28px;">Honors and Awards</h2>
                <p>• <strong>Outstanding Graduate of Tongji University</strong> - Tongji University</p>
                <p>• <strong>3rd Prize Scholarship for Outstanding Students</strong> - Tongji University</p>
                <!-- <p>• <strong>2nd Prize of the Mathercup Mathematical Modeling Contest</strong> - Chinese Society of Optimization, Overall Planning and Economic Mathematics</p> -->
                <p>• <strong>2nd Prize (Team Award) in RoboMaster Robotic Competition</strong> - DJI</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 20px 0 20px 0;">
                <h2 style="font-size: 28px;">Service</h2>
                <p>• <strong>Conference Reviewer</strong> - NeurIPS 2024, ICLR 2025, AISTATS 2025, CVPR 2025</p>
                <p>• <strong>Technical Program Committee</strong> - HCV Workshop 2024</p>
                <p>• <strong>Conference Volunteer</strong> - ICRA 2025</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
                <hr style="border: none; height: 1px; background-color: #e0e0e0; margin: 20px 0 20px 0;">
                <h2 style="font-size: 28px;">Teaching</h2>
                <p>• <strong>UVa CS 4710 - Artificial Intelligence</strong> - Teaching Assistant (Fall 2023)</p>
                <p>• <strong>UVa CS 2100 - Data Structures and Algorithms</strong> - Teaching Assistant (Spring 2024)</p>
                <p>• <strong>UVa ECE 4380 - AI Hardware</strong> - Teaching Assistant (Fall 2024)</p>
              </td>
            </tr>
          </tbody></table>

        
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template borrowed from <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>

<style>
  .no-gap {
    margin: 0;
    padding: 0;
  }
</style>
